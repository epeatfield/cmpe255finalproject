{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#weather = pd.read_csv('FL2018.csv', header=0, error_bad_lines=False, low_memory=False)\n",
    "#weather1 = pd.read_csv('CT2018.csv', header=0, parse_dates=False, low_memory=False)\n",
    "#weather = weather.append(weather1)\n",
    "#weather.reset_index(drop=True)\n",
    "#weather.to_csv('weather.csv')\n",
    "weather = pd.read_csv('weather.csv', header=0, error_bad_lines=False, low_memory=False)\n",
    "weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "weather['DATE'] = weather.DATE.astype(str)\n",
    "weather = weather.drop(weather.columns[[0]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read in data\n",
    "storms = pd.read_csv('StormEvents_details-ftp_v1.0_d2018_c20181017.csv.gz', compression='gzip', header=0)\n",
    "#weather = pd.read_csv('1531535.csv', header=0, error_bad_lines=False, low_memory=False)\n",
    "##Remove some unwanted columns\n",
    "weather = weather.drop(weather.columns[[28,30,31,32,33,35,36,38,39,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89]], axis=1)\n",
    "weather = weather.drop(weather.columns[[6,7,9]], axis=1)\n",
    "##add location columns to weather\n",
    "weather['COUNTY'] = np.nan\n",
    "weather['ST'] = np.nan\n",
    "weather.rename(columns={'STATION': 'WBAN'}, inplace=True)\n",
    "weather['STATION_NAME'] = weather['STATION_NAME'].astype(str).str[:-5]\n",
    "weather['WBAN'] = weather['WBAN'].str[5:].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read in data for stations and their locations\n",
    "locs = pd.read_fwf('emshr_lite_201810.txt', skiprows=[1], header=0)\n",
    "locs = locs[locs.CTRY_NAME == 'UNITED STATES'].reset_index(drop=True)\n",
    "col_list = ['WBAN', 'COUNTY', 'ST']\n",
    "locs = locs[col_list]\n",
    "locs = locs[pd.notnull(locs['COUNTY'])].reset_index(drop=True)\n",
    "locs = locs[pd.notnull(locs['WBAN'])].reset_index(drop=True)\n",
    "locs = locs[pd.notnull(locs['ST'])].reset_index(drop=True)\n",
    "locs['WBAN'] = locs['WBAN'].astype(str).str[:-2].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.merge(weather, locs, on=['WBAN']) ###Merge station location data with weather based on WBAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms.rename(columns={'CZ_NAME': 'COUNTY'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type = storms['EVENT_TYPE'].unique() ##add event ID\n",
    "event_type = pd.DataFrame(data=event_type, columns=['Weather']) ##Create table for the different types of storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_type['EventID'] = event_type.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storms = storms[storms.CZ_TYPE == 'C'].reset_index(drop=True) ##Only compare storms by county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ADD STATE COLUMN TO RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = weather_data[pd.notnull(weather_data['COUNTY_y'])].reset_index(drop=True)\n",
    "weather_data = weather_data[pd.notnull(weather_data['ST_y'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###REMOVE COUNTY X and ST X\n",
    "weather_data = weather_data.drop(weather_data.columns[[29,30]], axis=1)\n",
    "weather_data.rename(columns={'COUNTY_y': 'COUNTY'}, inplace=True)\n",
    "weather_data.rename(columns={'ST_y': 'ST'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REFORMAT DATE AND TIME DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['year'] = np.nan\n",
    "weather_data['BEGIN_DAY'] = np.nan\n",
    "weather_data['start_time'] = np.nan\n",
    "weather_data['month'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##split date into date and time, save time as time\n",
    "f = lambda x: x[\"DATE\"].rsplit(\" \", 1)[1]\n",
    "weather_data[\"start_time\"] = weather_data.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##split date to get day month year\n",
    "f2 = lambda x: x[\"DATE\"].rsplit(\" \", 1)[0]\n",
    "weather_data[\"BEGIN_DAY\"] = weather_data.apply(f2, axis=1)\n",
    "weather_data[\"year\"] = weather_data.apply(f2, axis=1)\n",
    "weather_data[\"month\"] = weather_data.apply(f2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = lambda x: x[\"BEGIN_DAY\"].split(\"-\", 2)[2]\n",
    "weather_data[\"BEGIN_DAY\"] = weather_data.apply(f3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f4 = lambda x: x[\"year\"].split(\"-\", 2)[0]\n",
    "f5 = lambda x: x['month'].split(\"-\",2)[1]\n",
    "weather_data['year'] = weather_data.apply(f4, axis=1)\n",
    "weather_data['month'] = weather_data.apply(f5, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fill in zeroes at front\n",
    "weather_data['month'] = weather_data['month'].apply(lambda x: x.zfill(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##combine month and year\n",
    "weather_data[\"BEGIN_YEARMONTH\"] = weather_data[\"year\"].map(str) + weather_data[\"month\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del weather_data['DATE']\n",
    "del weather_data['year']\n",
    "del weather_data['month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lambda x: x[\"start_time\"].split(\":\", 2)[0]\n",
    "z = lambda x: x[\"start_time\"].split(\":\", 2)[1]\n",
    "weather_data[\"start_time\"] = weather_data.apply(y, axis=1) + weather_data.apply(z, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove colon\n",
    "#d = lambda x: x['start_time'].replace(':','')\n",
    "#weather_data['start_time'] = weather_data.apply(d, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###drop monthly columns\n",
    "states = pd.read_csv('states.csv', delimiter=',', header=0)\n",
    "weather_data = pd.merge(weather_data, states, on=['ST'])\n",
    "weather_data['STATE'] = weather_data['STATE'].str.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['end_time'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['end_time'] = weather_data['start_time'].astype(int) + 100 ##add end time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now if END_TIME > 2399, subtract 2400\n",
    "test = weather_data.end_time > 2399\n",
    "column_name = 'end_time'\n",
    "weather_data.loc[test, column_name] -= 2400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['end_time'] = weather_data['end_time'].astype(str).apply(lambda x: x.zfill(4))\n",
    "weather_data['start_time'] = weather_data['start_time'].astype(str).apply(lambda x: x.zfill(4))\n",
    "storms['BEGIN_TIME'] = storms['BEGIN_TIME'].astype(str).apply(lambda x: x.zfill(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_data['COUNTY'] = weather_data['COUNTY'].apply(str)\n",
    "weather_data['STATE'] = weather_data['STATE'].apply(str)\n",
    "weather_data['BEGIN_YEARMONTH'] = weather_data['BEGIN_YEARMONTH'].apply(int)\n",
    "weather_data['BEGIN_DAY'] = weather_data['BEGIN_DAY'].apply(int)\n",
    "weather_data['start_time'] = weather_data['start_time'].apply(int)\n",
    "weather_data['end_time'] = weather_data['end_time'].apply(int)\n",
    "\n",
    "storms['COUNTY'] = storms['COUNTY'].apply(str)\n",
    "storms['STATE'] = storms['STATE'].apply(str)\n",
    "storms['BEGIN_YEARMONTH'] = storms['BEGIN_YEARMONTH'].apply(int)\n",
    "storms['BEGIN_DAY'] = storms['BEGIN_DAY'].apply(int)\n",
    "storms['BEGIN_TIME'] = storms['BEGIN_TIME'].apply(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storms.loc[(storms.STATE == 'CONNECTICUT') & (storms.COUNTY == 'HARTFORD')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.merge(weather_data, storms, on=['COUNTY', 'STATE','BEGIN_YEARMONTH', 'BEGIN_DAY']) ###merge columns based on county, state, yearmonth, day, and time range (starttime is between the weather_data range)\n",
    "##merge storm and weather based on county, state, month, day, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##only keep times that fall between start time and end time\n",
    "final = final[(final.start_time <= final.BEGIN_TIME) & (final.BEGIN_TIME <= final.end_time)] #filter via boolean series index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to csv to check data\n",
    "#final.to_csv(path_or_buf='testout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Remove Duplicates\n",
    "final = final.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final.drop_duplicates(subset=['EVENT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final['STATION_NAME']\n",
    "del final['LATITUDE']\n",
    "del final['LONGITUDE']\n",
    "del final['WBAN']\n",
    "del final['DAILYCoolingDegreeDays']\n",
    "del final['DAILYDeptFromNormalAverageTemp']\n",
    "del final['DAILYSnowDepth']\n",
    "del final['DAILYMaximumDryBulbTemp']\n",
    "del final['DAILYMinimumDryBulbTemp']\n",
    "del final['DAILYWeather']\n",
    "del final['CATEGORY']\n",
    "del final['COUNTY'], \n",
    "del final['ST']\n",
    "del final['BEGIN_DATE_TIME']\n",
    "del final['BEGIN_DAY']\n",
    "del final['BEGIN_LAT']\n",
    "del final['BEGIN_LOCATION']\n",
    "del final['BEGIN_LON']\n",
    "del final['BEGIN_RANGE']\n",
    "del final['BEGIN_TIME']\n",
    "del final['BEGIN_YEARMONTH']\n",
    "del final[ 'start_time'],final[ 'STATE'],final['end_time'],final['END_YEARMONTH'], final['END_DAY'],final['END_TIME']\n",
    "del final[ 'EPISODE_ID'],final[ 'STATE_FIPS'],final['YEAR'],final['MONTH_NAME'], final['EVENT_ID'],final['CZ_TYPE'], final['CZ_FIPS']\n",
    "del final[ 'WFO'],final[ 'CZ_TIMEZONE'],final['END_DATE_TIME'],final['INJURIES_DIRECT'], final['INJURIES_INDIRECT'],final['DEATHS_DIRECT'], final['DEATHS_INDIRECT']\n",
    "del final[ 'DAMAGE_PROPERTY'],final[ 'DAMAGE_CROPS'],final['SOURCE'],final['MAGNITUDE_TYPE'], final['FLOOD_CAUSE'],final['TOR_F_SCALE'], final['TOR_LENGTH']\n",
    "del final[ 'TOR_WIDTH'],final[ 'TOR_OTHER_WFO'],final['TOR_OTHER_CZ_STATE'],final['TOR_OTHER_CZ_FIPS'], final['TOR_OTHER_CZ_NAME'],final['BEGIN_AZIMUTH'], final['END_RANGE']\n",
    "del final[ 'END_AZIMUTH'],final[ 'END_LOCATION'],final['END_LAT'],final['END_LON'], final['EPISODE_NARRATIVE'],final['EVENT_NARRATIVE'], final['DATA_SOURCE']\n",
    "del final['HOURLYPressureChange'], final['HOURLYPressureTendency'], final['HOURLYSeaLevelPressure']\n",
    "final = final.fillna(0)\n",
    "final.loc[final.HOURLYPrecip == 'T', ['HOURLYPrecip']] = 0\n",
    "final.loc[final.HOURLYWindDirection == 'VRB', ['HOURLYWindDirection']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = final['EVENT_TYPE']\n",
    "del final['EVENT_TYPE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NAIVE BAYES\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
